This chapter is my notes from a Udemy Course called \textbf{Deep Reinforcement Learning 2.0}. During the course I will see theory notions but implementations of some AI that keeps learn. I will face Q-Learning and other cool stuff.

The structure of the course will be:
\begin{itemize}
\item Extra theory
\item Foundamentals
\item Twin Delayed DDPG - Theory
\item Twin Delayed DDPG - Implementation
\end{itemize}

\section{Resources}
\url{https://arxiv.org/pdf/1802.09477.pdf}. It is the paper that we will discuss dring the course


\section{Extra theory: basic knowledge}

In this section we will see what is reinforcement learning, Bellman equation, Markov decision process, plans,  living penalty, q-learning, temporal difference.

\subsection{Reinforcement Learning}
 The main ingredients of a RL systems are:
 \begin{itemize}
 \item Agent: the AI that has to fulfill  a task by taking actions
 \item Environment: the place where Agent lives
 \item Action and rewards: mechanism that make Agent do thing and discriminate if it is right or not. Action are hard coupled with time.. a correct action in a wrong time leads a bad rewards.
 \end{itemize}
 
 Training AI is like training a dog, if it does right it get a reward otherwise nothing.
 
 \begin{figure}
    \centering
    \includegraphics[scale=0.65]{img/reinforcementlearning01.png}
    \caption{Overview of Reinforcement Learning system}
    \label{img:RL-System}
\end{figure}

RL create RL agents that differs from Pre-Programmed Agents by the knowledge they have inside before start learning. The PPAgents have a series of rule that describe all actions they can do, in which case etc etc... while RL has to figure out those rule, his own behavior.

\subsection{BellMan Equation}

Notions:
\begin{itemize}
 \item $s$ the state of the agent
 \item $a$ action that agent can make
 \item $R$ reward agent gets for entering in a State
 \item $y$ Discount. 
 \end{itemize} 
 
  \begin{figure}
    \centering
    \includegraphics[scale=0.35]{img/rl-maze.png}
    \caption{RL - Maze task}
    \label{img:RL-Maze}
\end{figure}

 Before start let's define a task. We have an agent (the robot) in a maze composed by a grid with three different kinds of cells:
 \begin{itemize}
 \item Empy: the withe cell where agent is free to go
 \item Grey: the wall, agent cant cross this cell
 \item Red: The fire, agent can go there but it loses
 \item Green: the target, agent can go there and it wins
 \end{itemize}
 You can see this amazing maze in the picture \ref{img:RL-Maze}
 The agent can do 4 actions:
 \begin{itemize}
 \item Go Up
 \item Go Down
 \item Go Right
 \item Go Left
 \end{itemize}

\textbf{Basic Idea Of Bellman equation}. When agent goes to the green cell it gets a +1 rewards. It likes rewards, it is addicted, it cant live without so it think: "how i get the reward?" in order to be able to get it each time. So it learns that from the previous cell (3,3) it has to just move right (4,3) and he get the +1. Now each time it goes in (3,3) he knows how to get reward. He is happy but he could be more happy, more addicted. He start to think: "how i get here in the (3,3)" in order to be able to get this cell every time. So it learn for example that it can reach this cell from (3,2). Then it ask the same until it has learned a path from the start point to the green cell. Now it is really happy. 

\textbf{The bellman equation}. This is the bellman equation:
\begin{equation}
V(s) = max_a(R(s,a) + yV(s'))
\end{equation}
This equation gives a value $V$ to a State $s$ reached by the agent. The value is based on the best action it can take from that state. Let dig into it. We know agent in a state can take actions $a$, in our case it can chose between 4 actions each of them leads the action is a new state $s'$. Each action, took in a state has a reward for example if agent is in (3,3) and it goes right it get +1 as reward, if it is in (3,2) and it goes right it get -1 as reward because it ends in the fire. So each action, according to the current state, leads to different rewards $R(a,s)$. In the bellman equation we select the action that give the maximum value. So we cycle on all possible action computing the reward $R(s,a)$ and we select the action that brings agent to the greatest reward. We also consider the value of the reached state. Take an action brings agent from $s$ to $s'$ so the value of $s'$ must be taken in account $yV(s')$. So for each action we compute the reward $R(s,a)$ plus the value of the reached state $yV(s')$ and we select the action that gives the agent the greatest value because we want agent be optimal, so we can consider just best solutions.

\textbf{Let's do some example}
Let's compute the $V(s)$ of the (3,3) using a $y = 0.9$.
$V((3,3)) = R((3,3), goRight) + yV((3,4))$. We already know the best action because it is the only action that leads agent to the target (3,4). So considering this action we compute the value.

$V((3,3)) = 1 + y0$ V(3,4) is zero cause it is the target cell and we don't need it to have a value
So $V((3,3)) = 1$

We can repeat this for the V(2,3), we know the best action is go right so $V(2,3) = 0 + 0.9 * 1 = 0.9$. We can repeat the it for each cell.

\begin{figure}
    \centering
    \includegraphics[scale=0.35]{img/rl-maze-be.png}
    \caption{RL - Maze task with values from bellman equation}
    \label{img:RL-Maze}
\end{figure}

It easy to see that value decreases according to the distance to the target cell (4,3).

\subsection{Stochastic variable... OMG}
Untill now we speak about the maze a a very kind one. U want to go up, then u go up with 100\% possibility. But this is not how real life, out there, works. Let's add some randomness, let's create a stochastic maze where the action is a stochastic process. For example in cell (2,3) ground is broken so if u want to go up you have: 
\begin{itemize}
\item 80\% of probability to go up
\item 10\% of fall down
\item 10\% of fall in the cell with the hell's fire and die.
\end{itemize}

This is a \textbf{Markov Process} because the state $s'$ is expressed just according to the current state and not the previous decision of the agent. (Check the chapter about Markov Process).

That is how real work works. Our robot, that is addicted wish to get to the cell (3,3) because it perfectly know how to reach is drug from there. So from (3,2) decide to go up but it risk to fail and worsening his position. So the value of this cell must decrease.

Let's define the new Bellman Equation considering the Markov Decision Process:
\begin{equation}
V(s) = max_a(R(s,a) + y \sum_{n} P(s,a,s'_n)V(s'_n))
\end{equation}
Let's explain that, now agent can end in different states $s'_1 s'_2 ... s'_n$ and it can end there with a certain probability. Let's define P() as a function that gives to us this probability:
$P(s,a,s'_n)$ gives to us the probability from $s$ to reach $s'_n$ by action $a$. For each possible state $s'_n$ we need to compute this probability and multiply it with the value of that state and sum all values obtained.

Let's do an example. Let's compute the value of (3,2) considering the hard ground.
$V((3,2)) = 0 + 0.9 * ((0.8 * 1) + (0.1 * 0.73) + (0.1 * -1)) = 0.6957$

\begin{figure}
    \centering
    \includegraphics[scale=0.35]{img/rl-maze-stoch.png}
    \caption{RL - Maze task with all values changed because in each cell there is a probability to fall in two wrong cells}
    \label{img:RL-Maze-stoch}
\end{figure}

From pics \ref{img:RL-Maze-stoch} it's possbile to see the new cell values considering the probability to end in the wrong cells (80\%, 10\%, 10\%). That is why the agent in the cell (3,2) wont try to go up because he know that he have 10\% of probability to end in the fire an die. But it is more convenient for him to try to go in the wall an stay in the cell (3,2) but have a 20\% of probability to fall in a better cell without risk to end in the fire. It's possible to see it in the picture \ref{img:RL-Maze-plan}.

\begin{figure}
    \centering
    \includegraphics[scale=0.35]{img/rl-maze-plan.png}
    \caption{RL - Maze task with all values changed because in each cell there is a probability to fall in two wrong cells. We can see the actions that agent will try to take in each cell}
    \label{img:RL-Maze-plan}
\end{figure}

\subsection{Rewarding}

In our maze, by now, agent get two rewards the +1 if it arrives in the (4,3) cell (the green) or -1 if it ends in the fire. But what takes the agent away from waste time? Maybe it wants to walk a bit before get the reward? But we want agent be more addicted possible, we want it suffering the pain of hell if it doesn't get fast his drug!! Let's introduce \textbf{The Living Penalty}, a negative reward that it gets each time it take and actions. More action it takes more penalty if collects. Using this it select different action, for example it stops to try to go in the wall, has it does without penalty. You can see this differences in the picture \ref{img:RL-Maze-LP} 

\begin{figure}
    \centering
    \includegraphics[scale=0.35]{img/rl-maze-LP.png}
    \caption{RL - Actions for the maze with different Living Penalty}
    \label{img:RL-Maze-LP}
\end{figure}

\subsection{Q-Learning}
By now, using bellman equation, the best action is chosen by knowing the value of a state and selecting the action that brings agent it that state. In Q-Learning approach the idea is to evaluate directly the action using a \textit{Quality function} to measure the quality of an action $Q(s, a)$ where $s$ is the state of the agent  and $a$ is the action.
The $Q(s,a)$ equation come from this equation:
\begin{equation}
Q(s,a) = R(s,a) + y \sum_{n} P(s,a,s'_n)V(s'_n)
\end{equation}

As we can see it is identical to the final piece of bellman equation. But we still have $V(s)$ in our equation and we don't want it because it focuses on value of state and we want focus on value of possible action:

\begin{equation}
Q(s,a) = R(s,a) + y \sum_{n} P(s,a,s'_n) max_{a'}Q(s'_n, a')
\end{equation}

\subsection{Temporal Difference}

Now it obvious ask ourselves  if the same action at different time can have different value or precisely different q-function results. Let's introduce the temporal difference concept.

\textbf{Intuition}. Let's say our agent is for the first time in the maze. Let's place him in (1,1) at the very start. Now he can go in both direction (2,1) o (1,2). By now he has no idea of the path for the reward so he evaluates them equally: $Q(a_0, s) = Q(a_1,s)$. After some exploration he is back in the first cell (1,1). The cell is the same as before, the possible actions are the same as before but now he knows the best path so $Q(a_0, s) > Q(a_1,s)$. That means that $Q_{t_0}(a_0, s) < Q_{t_n}(a_0,s)$ because agent learned the best path. The same action from the same cell can have different importance according to the time it is performed because agent learn at each time.

Let's define this concept introducing $t$ the time:
\begin{equation}
Q_{t_1}(s,a) = Q_{t_0}(s,a) + { Q_{t_1}(s,a) - Q_{t_0}(s,a) }
\end{equation} 
From the above equation we can see that the $Q$ at time $t$ is the $Q$ at time $t-1$ plus the difference between them. Very easy .. doesn't it?. This difference is what agent has learned.
Let's call $Q_{t_1}(s,a) - Q_{t_0}(s,a) = TD(s,a)$ the temporal difference between the same action $a$ in the same state $s$ but in different times. We can now rewrite the equation as:
\begin{equation}
Q_{t_1}(s,a) = Q_{t_0}(s,a) +\alpha TD(s,a)
\end{equation}
$\alpha$ is a parameter that tell to the agent how much care to the past $Q_{t_0}$ or to the knowledge learned $TD$
In order to better understand let's expand the equation:
\begin{equation}
Q_{t_1}(s,a) = Q_{t_0}(s,a) + \alpha(R(s,a) + max_a Q(s',a') - Q_{t_0}(s,a))
\end{equation}
From this equation we can see that if $\alpha$ is set to 1 we have this:
\begin{equation}
Q_{t_1}(s,a) = R(s,a) + max_a Q(s',a')
\end{equation}
So the entire old value of $Q_{t_0}$ is replaced be the new value that comes from what agent has learned ( that is inside: $max_a Q(s',a')$). Otherwise, if we put $\alpha$ to 0 we 
 have:
 \begin{equation}
Q_{t_1}(s,a) = Q_{t_0}(s,a)
\end{equation}
From this equation we can see that nothing has changed so agent has learned nothing. This means that $\alpha$ is a coefficient that weight how much agent has to update its knowledge according to his new discoveries. Let's call $\alpha$ \textbf{learning rate} and this process \textbf{Q-Learning Process}. At the beginning of this process $Q(s,a)$ will be very low (or wrong) and the $TD$ very high. After some passages, when agent has learned the optimal path so the optimal action $Q(s,a)$ will be high (or optimal) and $TD$ will be low because agent can't learn something new.

\section{Deep Q-Learning}
\begin{figure}
    \centering
    \includegraphics[scale=0.45]{img/rl-deepql1.png}
    \caption{RL - Deep Neural Network overview for deep q-learning}
    \label{img:RL-deep-ql}
\end{figure}

The idea of \textbf{Deep Q-Learning} is to use a neural network to learn the best action to do in a given place. So the model should take in input $s$ expressed as a pair $<x_0, x_1>$ and it tries to predict the right action. So it gives in output a vector $<q_0, q_1, q_2, q_3>$ that represent the q-learning value associated to each action. \textit{This algorithm (deep q-learning) can be applied just when you have discrete actions like in this case.}

\subsection{Experience Replay}

The agent performs some actions and save them in a memory as experience. Then it select some of the past action, using a uniform distribution, and evaluate them. 

\texttt{To read: Prioritized Experience Replay - google deepmind 2016}

Experience Replay has different implementation so we have to define some policies like: max number of experience to save, how to discharge past experience for new ones, how to select them etc etc. 

\subsection{Action Selection Policy}
%% to do

\subsection{Deep Q-Learning: The algorithm}

First of all we need to put agent in a context. Then we need to define \textbf{Experience Replay Policy} and we are ready to start. So our agent will be in a state $s$ and he can do the same 4 actions: up, down, right, left. So he will try to do the best action predicting the Q-Value for each possible action. Let's say he predicts $[Q^p(s,up), Q^p(s,down), Q^p(s,right),Q^p(s,left)]$. The first time he will predict random value for instance: $[0.1, 0.1, 0.4, 0.05]$. According to our action selection policy he will select one action to do, let's say it select the one with the best score so: right. Now agent moves right and enters in a new state $s'$. Let's say he also get a reward $r = 1$ so now he can compute a better q-value for the action $right$ from state $s$: $Q^t(s,right) = R(s, right) + ymax_aQ(s', a) = r + 0 = 1$. Now agent can save in memory the experience using the tuple $(state = s, action = right, reward = r, end_state = s')$, for clarity let's express this tuple using time because agent can experience the same situation more times so  $(state_t = s, action_t = right, reward_t = r, end_state_t = s')$. He also save for that experience the predicted q-values.


It's time for the agent to learn. He predicted for this action the q-value $Q^p_{right} = 0.4$ then he performed the action and he discovered that a more precise q-value was $Q^t(s,right) = 1$ so he can use the loss function $Loss(Q^p, Q^t) = (Q^t - Q^p)^2$ and than he can back-propagate the gradient of the function to update weights as in a normal Neural Network.

 Let's add a new complexity, the memory. When agent has to learn he doesn't use just that experience but he select from his memory also N - 1 past experiences $e_0, e_1,..., e_{n - 1}$. He adds the new experience he has just performed and he get a pool of N experiences. Now it try to learn also form those experiences using the loss function:
 \begin{equation}
 Loss = \frac{1}{2} \sum_e ((Q^t(s,a) - Q^p_e(s,a))^2
 \end{equation}
 \newpage
 \section{Formula Summary}
 Bellman Equation:
 \begin{equation}
 V(s) = max_a(R(s,a) + yV(s'))
 \end{equation}
 Bellman Equation in a Markov Decision Process:
 \begin{equation}
 V(s) = max_a(R(s,a) + y \sum_n P(s,a,s'_n)V(s'_n))
 \end{equation}
 Q-Learning Equation:
 \begin{equation}
 Q(s,a) = R(s,a) + y \sum_n P(s,a,s'_n)V(s'_n) = R(s,a) + y \sum_n P(s,a,s'_n)max_{a'}Q(s'_n, a')
 \end{equation}
 Q-Learning Equation simplified (\textit{This equation substitutes the previous just because it is shorter, for simplicity}):
  \begin{equation}
 Q(s,a) = R(s,a) + ymax_{a'}Q(s'_n, a')
 \end{equation}
 
 
 
 Temporal Difference:
 \begin{equation}
 TD(s,a) = Q_{t1}(s,a) - Q_{t0}(s,a)
 \end{equation}
 \begin{equation}
 Q_{t1}(s,a) = Q_{t0}(s,a) +\alpha(Q_{t1}(s,a) - Q_{t0}(s,a)) =  Q_{t0}(s,a) +\alpha TD(s,a)
 \end{equation}
 \begin{equation}
 Q_{t_1}(s,a) = Q_{t_0}(s,a) + \alpha(R(s,a) + max_a Q(s',a') - Q_{t_0}(s,a))
 \end{equation}
 