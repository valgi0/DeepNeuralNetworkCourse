\babel@toc {english}{}
\contentsline {chapter}{\numberline {1}Why this documents}{1}{chapter.1}% 
\contentsline {section}{\numberline {1.1}Where Information come from}{1}{section.1.1}% 
\contentsline {section}{\numberline {1.2}Chapters organization}{1}{section.1.2}% 
\contentsline {section}{\numberline {1.3}Notation}{1}{section.1.3}% 
\contentsline {chapter}{\numberline {2}Has someone said Math?}{3}{chapter.2}% 
\contentsline {section}{\numberline {2.1}Linear Algebra OMG}{3}{section.2.1}% 
\contentsline {subsection}{\numberline {2.1.1}Vector Space}{3}{subsection.2.1.1}% 
\contentsline {subsection}{\numberline {2.1.2}Linear Independence}{4}{subsection.2.1.2}% 
\contentsline {subsection}{\numberline {2.1.3}Spanning a Vectors space}{4}{subsection.2.1.3}% 
\contentsline {subsection}{\numberline {2.1.4}Some functions}{5}{subsection.2.1.4}% 
\contentsline {section}{\numberline {2.2}Linear Transformation}{5}{section.2.2}% 
\contentsline {subsection}{\numberline {2.2.1}Eigenvalues eigenvectors}{6}{subsection.2.2.1}% 
\contentsline {subsubsection}{\nonumberline How to compute them}{6}{section*.1}% 
\contentsline {section}{\numberline {2.3}Probability}{7}{section.2.3}% 
\contentsline {subsection}{\numberline {2.3.1}Conditional Probability}{7}{subsection.2.3.1}% 
\contentsline {paragraph}{\nonumberline Exercise}{8}{section*.2}% 
\contentsline {subsection}{\numberline {2.3.2}Chain rule of probability}{8}{subsection.2.3.2}% 
\contentsline {subsection}{\numberline {2.3.3}Marginal Probability}{8}{subsection.2.3.3}% 
\contentsline {subsection}{\numberline {2.3.4}Relationships between variables}{9}{subsection.2.3.4}% 
\contentsline {chapter}{\numberline {3}Let's start}{11}{chapter.3}% 
\contentsline {section}{\numberline {3.1}Single Neuron}{11}{section.3.1}% 
\contentsline {subsection}{\numberline {3.1.1}Example of neuron}{11}{subsection.3.1.1}% 
\contentsline {section}{\numberline {3.2}Transfer functions}{11}{section.3.2}% 
\contentsline {subsubsection}{\nonumberline Diagonalization}{12}{section*.5}% 
\contentsline {section}{\numberline {3.3}Multi-input Neuron}{12}{section.3.3}% 
\contentsline {subsection}{\numberline {3.3.1}Example}{13}{subsection.3.3.1}% 
\contentsline {section}{\numberline {3.4}Multi-neurons}{13}{section.3.4}% 
\contentsline {section}{\numberline {3.5}Multi Layers}{14}{section.3.5}% 
\contentsline {section}{\numberline {3.6}Recurrent Neural Network}{14}{section.3.6}% 
\contentsline {subsection}{\numberline {3.6.1}Keep in mind}{15}{subsection.3.6.1}% 
\contentsline {section}{\numberline {3.7}Some Applications}{15}{section.3.7}% 
\contentsline {subsection}{\numberline {3.7.1}Perceptron}{15}{subsection.3.7.1}% 
\contentsline {chapter}{\numberline {4}Basic Concepts about learning rules}{17}{chapter.4}% 
\contentsline {section}{\numberline {4.1}Learning rules for Perceptron Architecture}{17}{section.4.1}% 
\contentsline {subsection}{\numberline {4.1.1}Perceptron Network again}{17}{subsection.4.1.1}% 
\contentsline {subsection}{\numberline {4.1.2}Decision Boundary for Multiple Neurons}{18}{subsection.4.1.2}% 
\contentsline {section}{\numberline {4.2}Learning rules for Perceptron Network}{19}{section.4.2}% 
\contentsline {subsection}{\numberline {4.2.1}Example}{19}{subsection.4.2.1}% 
\contentsline {section}{\numberline {4.3}Perceptron Network Limitation}{20}{section.4.3}% 
\contentsline {chapter}{\numberline {5}Training a model}{21}{chapter.5}% 
\contentsline {section}{\numberline {5.1}Objective function: overview}{21}{section.5.1}% 
\contentsline {section}{\numberline {5.2}Different kinds of Loss Function}{21}{section.5.2}% 
\contentsline {section}{\numberline {5.3}Optimizer}{21}{section.5.3}% 
\contentsline {subsection}{\numberline {5.3.1}BackPropagation and lr}{22}{subsection.5.3.1}% 
\contentsline {chapter}{\numberline {6}Markov Decision Processes}{25}{chapter.6}% 
\contentsline {chapter}{\numberline {7}Generative Models}{27}{chapter.7}% 
\contentsline {section}{\numberline {7.1}Energy based model}{27}{section.7.1}% 
\contentsline {subsection}{\numberline {7.1.1}How it work?}{27}{subsection.7.1.1}% 
\contentsline {section}{\numberline {7.2}Boltzman Machine}{28}{section.7.2}% 
\contentsline {subsection}{\numberline {7.2.1}Restricted Boltzman Machine}{29}{subsection.7.2.1}% 
\contentsline {chapter}{\numberline {8}EXTRA: Neural Symbolic Integration}{31}{chapter.8}% 
\contentsline {section}{\numberline {8.1}Introduction to Neural Symbolic Integration}{31}{section.8.1}% 
\contentsline {subsection}{\numberline {8.1.1}Logic As Constrain}{32}{subsection.8.1.1}% 
\contentsline {subsection}{\numberline {8.1.2}Logic As Task}{32}{subsection.8.1.2}% 
\contentsline {section}{\numberline {8.2}Neural Symbolic Cognitive Agent (NSCA)}{32}{section.8.2}% 
\contentsline {subsection}{\numberline {8.2.1}References}{32}{subsection.8.2.1}% 
\contentsline {chapter}{\numberline {9}Reinforcement Learning}{33}{chapter.9}% 
\contentsline {section}{\numberline {9.1}Resources}{33}{section.9.1}% 
\contentsline {section}{\numberline {9.2}Extra theory: basic knowledge}{33}{section.9.2}% 
\contentsline {subsection}{\numberline {9.2.1}Reinforcement Learning}{33}{subsection.9.2.1}% 
\contentsline {subsection}{\numberline {9.2.2}BellMan Equation}{34}{subsection.9.2.2}% 
\contentsline {subsection}{\numberline {9.2.3}Stochastic variable... OMG}{36}{subsection.9.2.3}% 
\contentsline {subsection}{\numberline {9.2.4}Rewarding}{37}{subsection.9.2.4}% 
\contentsline {subsection}{\numberline {9.2.5}Q-Learning}{39}{subsection.9.2.5}% 
\contentsline {subsection}{\numberline {9.2.6}Temporal Difference}{39}{subsection.9.2.6}% 
\contentsline {section}{\numberline {9.3}Deep Q-Learning}{40}{section.9.3}% 
\contentsline {subsection}{\numberline {9.3.1}Experience Replay}{40}{subsection.9.3.1}% 
\contentsline {subsection}{\numberline {9.3.2}Action Selection Policy}{41}{subsection.9.3.2}% 
\contentsline {subsection}{\numberline {9.3.3}Deep Q-Learning: The algorithm}{41}{subsection.9.3.3}% 
\contentsline {section}{\numberline {9.4}Formula Summary}{42}{section.9.4}% 
